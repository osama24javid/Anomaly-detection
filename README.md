# Anomaly-detection
**DATA GATHERING**
The collection of data for the suggested modelâ€™s training and testing is the initial phase in this research. the Three datasets are used to evaluate the model: CIC-IDS 2017.
**CIC-IDS2017:**
Since it was introduced, the CICIDS2017 dataset has been widely used by researchers to design new models and algorithms. Dataset, which was collected by the Canadian
Institute of Cybersecurity, consists of eight files contains five days of normal and attack data There are a total of 2,40,394 distinct instances in the dataset, with 83 features.
**DATA PRE-PROCESS**
Data pre-processing is an essential step into transforming raw data into meaningful and efficient data that can be used by ML algorithms. The first technique used in this process is data normalization, specifically min-max normalization,
which scales all values of the attributes to a range of 0 to 1. The second technique is label encoding, which converts all string values in the data-set to 0s and 1s based on the output class label. The third technique used is Synthetic Minority
Oversampling Technique (SMOTE), which oversamples the few classes in the dataset to create a balance distribution and use the correlation to extract the best 30 features. In our preprocessing method, we apply different techniques to generate the best training dataset for the ensemble classifiers. As a result, we have three balanced sample datasets.
**MACHINE LEARNING ALGORITHMS**
In the proposed method, we use the ensemble voting classifier that uses Decision Tree Random Forest and XGBoost machine learning algorithms. These are discussed below:
**1) Decision Tree**
**2) Random Fores**
**3) XGBoost**
**PROPOSED ENSEMBLE VOTING CLASSIFIER**
Classifier combines many base model predictions, frequently decision trees, to get a more precise final forecast. Using a process called majority voting, the forecasts of the basemodels are combined to create the final projection. The same train data is used to train the basic models. The two funda-
mental subcategories of ensemble polling classifiers are hard voting and soft voting. Class label that the base models most commonly predict is the outcome in hard voting (e.g., normal or malicious). Predictions are generated by the basic models
as class labels (e.g., normal or malicious). The underlying models produce probability estimates as predictions for softvoting, with the class label with the greatest average probability serving as the final projection. A
Results Discussing the Report.
